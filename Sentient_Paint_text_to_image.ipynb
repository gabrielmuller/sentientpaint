{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentient Paint: text to image",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielmuller/sentientpaint/blob/main/Sentient_Paint_text_to_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFcTVijK3guK"
      },
      "source": [
        "# Sentient Paint: text to image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlxnCpAtHgm7"
      },
      "source": [
        "Generate images from text using CLIP and feature visualization techniques (Fourier basis, color correlation, transformation robustness).\n",
        "\n",
        "[CLIP: Connecting\n",
        "Text and Images](https://openai.com/blog/clip/)\n",
        "\n",
        "[Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
        "\n",
        "Creates images larger than the model's input size. Every epoch, random square crops are taken and resized to 224x224. CLIP computes \n",
        " similarity values for each crop, which are then summed up into the loss value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYE0MCBaIG-w"
      },
      "source": [
        "# How to use\n",
        "\n",
        "*   Go to *Parameters > Prompt* section\n",
        "*   Replace the prompt text with your own\n",
        "*   Run everything (Ctrl + F9)\n",
        "\n",
        "The first run will install dependencies. This takes a few minutes.\n",
        "\n",
        "After that, images will appear in *Optimize image* as they are computed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxWbDwfsUiz"
      },
      "source": [
        "# Advanced usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAZw-teQsZIb"
      },
      "source": [
        "## High resolution images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3K7LCo-sux0"
      },
      "source": [
        "You can generate higher resolution images by changing image resolution. Fine details are added by optimizing for small crops of the image. Because of this, the resulting image can take long to generate, and look repetitive.\n",
        "\n",
        "For better results, try the following steps:\n",
        "\n",
        "*   Start optimizing with a lower resolution\n",
        "*   Wait until the image is stable, then stop optimization\n",
        "*   Run *Image resolution and smoothness* with increased resolution and less smoothness\n",
        "*   Run *Rescale image*\n",
        "*   Run *Optimize image* again\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu-pMvUmsdlE"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5xDrRvfwYP-"
      },
      "source": [
        "An image is optimized iteratively. This means we can save the image at every step and join them as frames of a video.\n",
        "\n",
        "Furthermore, we can apply a small transformation every frame, such as zoom, to create an infinite zoom effect.\n",
        "\n",
        "*   Go to *New video*\n",
        "*   Change `VIDEO_NAME` to some value and run the cell\n",
        "*   Run *Optimize image*\n",
        "*   Run *Download video*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgKMv-7KsjEc"
      },
      "source": [
        "## Style transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSJw2ngAtmlj"
      },
      "source": [
        "You can transfer a style to an image by setting it as the initial image, then optimizing it.\n",
        "\n",
        "\n",
        "\n",
        "*   Click the file icon on the left sidebar\n",
        "*   Click the upload file button to choose your image\n",
        "*   Right click the uploaded file and choose \"Copy path\"\n",
        "*   Go to the *Load image from file* section\n",
        "*   Paste the path and run the cell\n",
        "*   Run *Rescale image*\n",
        "*   Run *Optimize image*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scMn-0xtLFGw"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRWf2fwUCdPC"
      },
      "source": [
        "## Install torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd2A2kj8BCOv"
      },
      "source": [
        "import subprocess\n",
        "from re import findall\n",
        "version_out = subprocess.check_output(['nvcc', '--version']).decode()\n",
        "version = findall(r'release (.*),', version_out)[0]\n",
        "\n",
        "print('CUDA version', version)\n",
        "version_map = {\n",
        "    '10.0': '+cu100',\n",
        "    '10.1': '+cu101',\n",
        "    '10.2': '+cu102',\n",
        "}\n",
        "\n",
        "suffix = version_map[version] if version in version_map else '+cu110'\n",
        "\n",
        "!pip install torch==1.7.1{suffix} torchvision==0.8.2{suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzREZutJ3cp4"
      },
      "source": [
        "## Install CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n0nJVfYEBZx"
      },
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruh7mxuPMBHR"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yqucVz3ctef"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.fft as fft\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLt853kkCrO3"
      },
      "source": [
        "## Load CLIP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se9fBnzCBgp4"
      },
      "source": [
        "import clip\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "model, preprocess = clip.load('ViT-B/32', device=DEVICE)\n",
        "\n",
        "# Width and height of model input\n",
        "CLIP_SIZE = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0xGdLwlvASM"
      },
      "source": [
        "# Feature visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_IQVLKlAFi"
      },
      "source": [
        "## Transformation robustness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dJCRO6pk3Wx"
      },
      "source": [
        "# Redefine CLIP normalization function so it can be backpropagated\n",
        "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                                 (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "def image_transform(image_size, big_picture_focus=1.8):\n",
        "  # Crop scale is random, so CLIP optimizes for both the big picture and\n",
        "  # fine details.\n",
        "  crop_ratio = CLIP_SIZE / min(image_size)\n",
        "  scale = crop_ratio + random.random()**big_picture_focus * (1-crop_ratio)\n",
        "  resize = lambda image: torch.nn.functional.interpolate(image,\n",
        "                                                         scale_factor=scale,\n",
        "                                                         mode='bilinear')\n",
        "  return transforms.Compose([\n",
        "      transforms.Lambda(clamp_image),\n",
        "      transforms.Lambda(resize),\n",
        "      transforms.RandomAffine(5, scale=(1.1, 1.1), resample=Image.BILINEAR),\n",
        "      transforms.RandomCrop(CLIP_SIZE),\n",
        "      normalize,\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaS1Ah5bCwpm"
      },
      "source": [
        "## Frequency weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6aau5tbPfGj"
      },
      "source": [
        "def generate_frequency_weights(image_size, smoothness):\n",
        "  # `fftfreq` is not available in torch 1.7, so let's use numpy's instead\n",
        "  import numpy as np\n",
        "\n",
        "  # Build a frequency weight matrix. Define how much of each frequency the\n",
        "  # gradient should have. Weight is inversely proportional to frequency.\n",
        "  vertical_freqs = torch.tensor(np.fft.fftfreq(SIZE[0]),\n",
        "                                device=DEVICE,\n",
        "                                dtype=torch.float32).unsqueeze(1)\n",
        "  horizontal_freqs = torch.tensor(np.fft.rfftfreq(SIZE[1]),\n",
        "                                  device=DEVICE,\n",
        "                                  dtype=torch.float32)\n",
        "\n",
        "  # Use vertical and horizontal frequency to calculate euclidean distance\n",
        "  shape = vertical_freqs.shape[0], horizontal_freqs.shape[0]\n",
        "  weights = torch.square(vertical_freqs).expand(shape) + \\\n",
        "                torch.square(horizontal_freqs).expand(shape)\n",
        "  weights = torch.sqrt(weights)\n",
        "\n",
        "  weights = torch.pow(weights, smoothness)\n",
        "\n",
        "  # Avoid division by zero\n",
        "  weights[0, 0] = weights[0, 1]\n",
        "\n",
        "  # Scale with the mean to compensate for the low pass\n",
        "  weights = weights.mean() / weights\n",
        "\n",
        "  print('Frequency weights computed.')\n",
        "  print(weights.shape)\n",
        "  return weights\n",
        "\n",
        "\n",
        "def balance_frequencies(grad, shape, weights):\n",
        "  # Use Fourier transform to get gradient's frequencies\n",
        "  grad_fft = fft.rfftn(grad, shape)\n",
        "\n",
        "  # Apply desired weights to frequencies\n",
        "  grad_fft *= weights\n",
        "\n",
        "  # Inverse transform back into gradient\n",
        "  return fft.irfftn(grad_fft, shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIC3C-8bDNHG"
      },
      "source": [
        "## Color correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lmEN9DzpagT"
      },
      "source": [
        "# Color correlation matrix precomputed using Cholesky decomposition on some\n",
        "# images from the CIFAR dataset\n",
        "COLOR_CORRELATION = torch.Tensor([[1.0000, 0.0000, 0.0000],\n",
        "                                  [0.9224, 0.3862, 0.0000],\n",
        "                                  [0.8182, 0.4037, 0.4094]]).to(DEVICE)\n",
        "\n",
        "\n",
        "def correlate_colors(image):\n",
        "  height = image.shape[2]\n",
        "  width = image.shape[3]\n",
        "\n",
        "  # Change shape to 3 x N matrix\n",
        "  image = torch.transpose(image, 1, 3)\n",
        "  image = image.reshape(-1, 3)\n",
        "\n",
        "  # Normalize image to unit mean and standard deviation\n",
        "  mean = image.mean(0, keepdim=True)\n",
        "  std = image.std(0, keepdim=True)\n",
        "  image = (image - mean) / std\n",
        "\n",
        "  # Apply correlation matrix\n",
        "  image = image @ COLOR_CORRELATION\n",
        "\n",
        "  # Restore mean and deviation\n",
        "  image = image * std + mean\n",
        "\n",
        "  # Restore shape\n",
        "  image = image.reshape(1, width, height, 3)\n",
        "  image = torch.transpose(image, 1, 3)\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiA8AuezPp-6"
      },
      "source": [
        "## Other functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHjfwpIcXa62"
      },
      "source": [
        "# Keep image inside valid range\n",
        "def clamp_image(image):\n",
        "  return torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def display_image(image):\n",
        "  with torch.no_grad():\n",
        "    pil_image = transforms.ToPILImage()(clamp_image(image[0]))\n",
        "    display(pil_image)\n",
        "\n",
        "\n",
        "def save(image, path, index):\n",
        "  with torch.no_grad():\n",
        "    pil_image = transforms.ToPILImage()(clamp_image(image[0]))\n",
        "    pil_image.save(path + str(index).zfill(4) + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZNullbMMbdx"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY7-iiBMJvXI"
      },
      "source": [
        "## Image resolution and smoothness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbj9-Rq6Mh1R"
      },
      "source": [
        "Note: if you want to create high resolution images, I recommend optimizing with a lower resolution, upscaling the image (see *Change image size* section), and then continue to optimize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2HrFBW-JuvB"
      },
      "source": [
        "# Replace this with your desired image resolution (height, width).\n",
        "SIZE = 480, 854\n",
        "\n",
        "# How much should lower frequencies dominate? Replace this with a value around\n",
        "# the [1.0, 2.0] range for the first pass, or less than 1.0 for a finer detail\n",
        "# pass.\n",
        "# A lower value means more detail but also more noise. \n",
        "# A high value is more natural but also blurrier.\n",
        "SMOOTHNESS = 1.7\n",
        "\n",
        "\n",
        "freq_weights = generate_frequency_weights(SIZE, SMOOTHNESS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7A9932WDIXY"
      },
      "source": [
        "## Prompt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PWSAHvAcQcG"
      },
      "source": [
        "# ENTER YOUR PROMPT HERE\n",
        "OPTIMIZE = '''Sentient Paint, a colorful surreal painting'''\n",
        "\n",
        "# Avoid undesirable features\n",
        "DEOPTIMIZE = '''text, writing, signature'''\n",
        "\n",
        "\n",
        "text = clip.tokenize([OPTIMIZE, DEOPTIMIZE]).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ihrlmfVNEVy"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdtCTEzVDiNp"
      },
      "source": [
        "## New video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1t1J8WLLWyy"
      },
      "source": [
        "Choose a video name to save images as frames for a video. Run *Download video* to generate a video from saved images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYBRMp5Yt84"
      },
      "source": [
        "# Enter a name for the video e.g. VIDEO_NAME = 'hello'\n",
        "VIDEO_NAME = ''\n",
        "\n",
        "if VIDEO_NAME:\n",
        "  VIDEO_FILENAME = VIDEO_NAME + '.mp4'\n",
        "  PATH = '/content/' + VIDEO_FILENAME + '/'\n",
        "\n",
        "  if not os.path.exists(PATH):\n",
        "    os.makedirs(PATH)\n",
        "\n",
        "# Transformation applied for every frame in the video\n",
        "def frame_transform(image, zoom_factor=1.01):\n",
        "  crop_size = int(SIZE[0] / zoom_factor), int(SIZE[1] / zoom_factor)\n",
        "\n",
        "  zoomed = transforms.CenterCrop(crop_size)(image)\n",
        "  zoomed = torch.nn.functional.interpolate(zoomed, size=SIZE, mode='bilinear')\n",
        "  return zoomed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aGeQZ37Down"
      },
      "source": [
        "## New image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZvEtYLNH_ZT"
      },
      "source": [
        "def generate_initial_image(freq_shape, image_shape, high_freq_amount=0.2):\n",
        "  # Generate random frequency and phase\n",
        "  shape = 1, 3, *freq_shape\n",
        "  random_fft = torch.rand(shape, device=DEVICE, dtype=torch.cfloat)\n",
        "  random_fft = random_fft * freq_weights\n",
        "\n",
        "  # Generate image from frequencies\n",
        "  image = fft.irfftn(random_fft, image_shape)\n",
        "\n",
        "  # Add some regular noise too. Seems to work well\n",
        "  high_freq_noise = torch.rand(1, 3, *image_shape).to(DEVICE) * high_freq_amount\n",
        "  image += high_freq_noise\n",
        "\n",
        "  # Reduce the image's range. Make it closer to gray\n",
        "  image = (image + 1.5) / 4.0\n",
        "\n",
        "  image = clamp_image(correlate_colors(image))\n",
        "  image.requires_grad = True\n",
        "  return image\n",
        "\n",
        "image = generate_initial_image(freq_weights.shape, SIZE)\n",
        "\n",
        "epoch = 0\n",
        "\n",
        "print('Initial image:')\n",
        "display_image(image)\n",
        "print(image)\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R2vOzZ4GT9B"
      },
      "source": [
        "## Optimize image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iJy2Q0xE0T5"
      },
      "source": [
        "# A higher learning rate will generate an image quicker, but the image may look\n",
        "# oversaturated and weird\n",
        "optimizer = torch.optim.Adam([image], lr=0.03)\n",
        "\n",
        "# How many samples (random crops) of the image we should take per batch.\n",
        "# This is limited by VRAM.\n",
        "N_SAMPLES_PER_BATCH = 80\n",
        "\n",
        "# Larger images need more samples.\n",
        "TARGET_N_SAMPLES = int(SIZE[0] * SIZE[1] / 8000)\n",
        "\n",
        "N_BATCHES = max(int(TARGET_N_SAMPLES / N_SAMPLES_PER_BATCH), 1)\n",
        "print(f'Optimizing {N_BATCHES * N_SAMPLES_PER_BATCH} samples per epoch')\n",
        "\n",
        "def train(image,\n",
        "          n_samples_per_batch,\n",
        "          n_batches,\n",
        "          big_picture_focus=1.8,\n",
        "          deoptimization_weight=0.2):\n",
        "  transform_image = image_transform(SIZE, big_picture_focus=big_picture_focus)\n",
        "\n",
        "  # Gradient accumulator, a running total of each batch's gradient\n",
        "  grad_acc = torch.zeros(1, 3, *SIZE, device=DEVICE)\n",
        "\n",
        "  for _ in range(n_batches):\n",
        "    samples = torch.zeros(n_samples_per_batch,\n",
        "                          3,\n",
        "                          CLIP_SIZE,\n",
        "                          CLIP_SIZE,\n",
        "                          device=DEVICE)\n",
        "    for i in range(n_samples_per_batch):\n",
        "      samples[i] = transform_image(image)\n",
        "\n",
        "    # CLIP computes the similarity between each sample and text\n",
        "    similarities, _ = model(samples, text)\n",
        "    loss = 0\n",
        "\n",
        "    # Add up all similarities into a loss value. The optimizer minimizes loss.\n",
        "    for similarity in similarities:\n",
        "      # Each sample has two similarity values: one for each prompt\n",
        "      good, bad = similarity\n",
        "      loss += deoptimization_weight * bad - (1 - deoptimization_weight) * good\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss /= n_samples_per_batch\n",
        "    loss.backward()\n",
        "    grad_acc += image.grad\n",
        "\n",
        "  grad_acc /= n_batches * n_samples_per_batch\n",
        "  grad_acc = balance_frequencies(grad_acc, SIZE, freq_weights)\n",
        "  grad_acc = correlate_colors(grad_acc)\n",
        "  image.grad = grad_acc\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss.item()\n",
        "\n",
        "\n",
        "for i in range(120):\n",
        "  loss = train(image, N_SAMPLES_PER_BATCH, N_BATCHES)\n",
        "  print('Epoch', epoch, 'Loss', loss)\n",
        "\n",
        "  if (epoch + 1) % 20 == 0:\n",
        "    display_image(image)\n",
        "\n",
        "  if VIDEO_NAME:\n",
        "    with torch.no_grad():\n",
        "      image.copy_(frame_transform(image))\n",
        "    save(image, PATH, epoch)\n",
        "  \n",
        "  epoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lr6TFRLNanZ"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUBNG-9dK6hb"
      },
      "source": [
        "## Show current image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWZjfH5GHhP0"
      },
      "source": [
        "display_image(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pk1fghjEFHC"
      },
      "source": [
        "## Download video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqfcowjqcm9K"
      },
      "source": [
        "GLOB = PATH + '*.png'\n",
        "OUT = PATH + VIDEO_FILENAME\n",
        "\n",
        "!ffmpeg -framerate 30 -y -pattern_type glob -i \"{GLOB}\" -vf \"pad=ceil(iw/2)*2:ceil(ih/2)*2\" -c:v libx264 -pix_fmt yuv420p \"{OUT}\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download(OUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8QFTeWLvrJ"
      },
      "source": [
        "## Load image from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7yrV8d_L8sg"
      },
      "source": [
        "# Enter path here e.g. '/content/hello.png'\n",
        "path = ''\n",
        "\n",
        "if path:\n",
        "  image = transforms.ToTensor()(Image.open(path).convert('RGB'))\n",
        "  image = image.unsqueeze(0).to(DEVICE)\n",
        "  image.requires_grad = True\n",
        "  print(image.shape)\n",
        "  epoch = 0\n",
        "else:\n",
        "  print('No file specified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyS2EaDqrTHa"
      },
      "source": [
        "If the output image resolution is not the same as the loaded image, run *Rescale image*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82ql8fNEjBY"
      },
      "source": [
        "## Rescale image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mk8YojpFwj2"
      },
      "source": [
        "print('Before', image.shape)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = torch.nn.functional.interpolate(image,\n",
        "                                          size=SIZE,\n",
        "                                          mode='bicubic').to(DEVICE)\n",
        "  image.requires_grad = True\n",
        "\n",
        "print('After', image.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2hC4G4F3SVo"
      },
      "source": [
        "## Check GPU model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mc96c_HeBbc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJw-RKs7Gx7X"
      },
      "source": [
        "## Free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQXZxu0G_6A"
      },
      "source": [
        "Try this if you run out of memory. If it doesn't work, reduce `N_SAMPLES_PER_BATCH` in *Optimize image*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNFNv4_bjJkt"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEzgVMb9PCE"
      },
      "source": [
        "# LICENSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLgZ3-DD9RJ6"
      },
      "source": [
        "The MIT License\n",
        "\n",
        "Copyright 2021 Gabriel Müller\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n"
      ]
    }
  ]
}